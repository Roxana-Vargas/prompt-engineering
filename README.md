# Prompt Engineering Toolkit

Un conjunto de herramientas para diseÃ±ar, probar y optimizar prompts usando diferentes tÃ©cnicas de prompt engineering.

## ğŸ¥ Demo del proyecto

<a href="https://youtu.be/sOLWpS_XFWA" target="_blank">
  <img src="https://raw.githubusercontent.com/Roxana-Vargas/prompt-engineering/refs/heads/main/Captura%20de%20pantalla%202025-12-01%20212744.png" 
       alt="Demo" 
       style="width:100%; max-width:800px; border-radius:12px;">
</a>

## ğŸ¯ CaracterÃ­sticas

- **MÃºltiples TÃ©cnicas de Prompting**: ImplementaciÃ³n de tÃ©cnicas avanzadas como:
  - Zero-Shot Prompting
  - Few-Shot Prompting
  - Chain-of-Thought (CoT)
  - ReAct (Reasoning + Acting)
  - Self-Consistency
  - Tree-of-Thoughts

- **Sistema de EvaluaciÃ³n**: Sistema completo para comparar y evaluar diferentes tÃ©cnicas de prompting
- **MÃ©tricas de Rendimiento**: AnÃ¡lisis de tiempo de ejecuciÃ³n, tokens utilizados y scores de calidad
- **Soporte Multi-Provider**: Compatible con OpenAI y Anthropic APIs
- **Ejemplos PrÃ¡cticos**: Casos de uso reales demostrando cada tÃ©cnica
- **ğŸ†• Dashboard Interactivo**: AplicaciÃ³n Streamlit con visualizaciones interactivas y grÃ¡ficos

## ğŸ“‹ Requisitos

- Python 3.8+
- API keys de OpenAI o Anthropic

## ğŸš€ InstalaciÃ³n

1. Clona el repositorio:
```bash
git clone https://github.com/Roxana-Vargas/prompt-engineering
```

2. Instala las dependencias:
```bash
pip install -r requirements.txt
```

3. Configura las variables de entorno:
```bash
cp .env.example .env
# Edita .env y agrega tus API keys
```

## ğŸš€ Inicio rÃ¡pido

### OpciÃ³n 1: AplicaciÃ³n Web con Streamlit (Recomendado) ğŸ¨

```bash
# Instalar dependencias (incluye Streamlit)
pip install -r requirements.txt

# Ejecutar aplicaciÃ³n web
streamlit run streamlit_app.py

# O en Windows:
run_streamlit.bat
```

La aplicaciÃ³n web te permite:
- ğŸ¯ Ejecutar benchmarks de forma interactiva
- ğŸ“Š Visualizar resultados con grÃ¡ficos interactivos
- ğŸ“ˆ Comparar tÃ©cnicas visualmente
- ğŸ“¥ Descargar resultados en CSV

### OpciÃ³n 2: LÃ­nea de Comandos

```bash
# Modo interactivo
python main.py --interactive

# Ejecutar ejemplos especÃ­ficos
python main.py --example math
python main.py --example comprehensive

# Ver todas las tÃ©cnicas disponibles
python main.py --list-techniques

```

## ğŸ’» Uso bÃ¡sico

### Ejemplo 1: ComparaciÃ³n de tÃ©cnicas

```python
from src.prompt_templates.techniques import get_technique
from src.utils.llm_client import get_client
from src.utils.evaluator import PromptEvaluator

# Inicializar cliente
client = get_client("openai", model="gpt-4")
evaluator = PromptEvaluator(client)

# Definir tarea
task = "Resuelve: Si tengo 5 manzanas y como 2, Â¿cuÃ¡ntas me quedan?"

# Construir prompts con diferentes tÃ©cnicas
zero_shot = get_technique("zero_shot")
cot = get_technique("chain_of_thought")

techniques = {
    "Zero-Shot": zero_shot.build_prompt(task),
    "Chain-of-Thought": cot.build_prompt(task)
}

# Comparar tÃ©cnicas
comparison = evaluator.compare_techniques(
    task=task,
    techniques=techniques,
    temperature=0.7
)

# Generar reporte
report = evaluator.generate_report(comparison)
print(report)
```

### Ejemplo 2: Chain-of-Thought para Razonamiento MatemÃ¡tico

```python
from examples.math_reasoning import main
main()
```

### Ejemplo 3: Few-Shot Learning para AnÃ¡lisis de Texto

```python
from examples.text_analysis import main
main()
```

## ğŸ“š TÃ©cnicas Implementadas

### Zero-Shot Prompting
Prompt directo sin ejemplos. Ãštil para tareas simples donde el modelo tiene conocimiento previo.

### Few-Shot Prompting
Incluye ejemplos de demostraciÃ³n para guiar al modelo. Mejora el rendimiento en tareas especÃ­ficas.

### Chain-of-Thought (CoT)
Fomenta el razonamiento paso a paso. Especialmente efectivo para problemas matemÃ¡ticos y lÃ³gicos.

### ReAct (Reasoning + Acting)
Combina razonamiento y acciones en un proceso iterativo. Ideal para tareas que requieren mÃºltiples pasos.

### Self-Consistency
Genera mÃºltiples caminos de razonamiento y selecciona la respuesta mÃ¡s consistente.

### Tree-of-Thoughts
Explora mÃºltiples estrategias de razonamiento en una estructura de Ã¡rbol.

## ğŸ“Š Estructura del proyecto

```
prompt-engineering/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ prompt_templates/
â”‚   â”‚   â””â”€â”€ techniques.py      # ImplementaciÃ³n de tÃ©cnicas
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py           # ConfiguraciÃ³n
â”‚       â”œâ”€â”€ llm_client.py       # Clientes LLM
â”‚       â””â”€â”€ evaluator.py        # Sistema de evaluaciÃ³n
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ math_reasoning.py       # Ejemplo: Razonamiento matemÃ¡tico
â”‚   â”œâ”€â”€ text_analysis.py        # Ejemplo: AnÃ¡lisis de texto
â”‚   â”œâ”€â”€ react_example.py        # Ejemplo: ReAct
â”‚   â””â”€â”€ comprehensive_comparison.py  # ComparaciÃ³n completa
â”œâ”€â”€ streamlit_app.py            # ğŸ†• Dashboard interactivo Streamlit
â”œâ”€â”€ main.py                     # Punto de entrada principal
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ run_streamlit.bat          # Script para ejecutar Streamlit (Windows)
â”œâ”€â”€ run_streamlit.sh           # Script para ejecutar Streamlit (Linux/Mac)
â””â”€â”€ README.md
```

## ğŸ“ Casos de uso

1. **Razonamiento MatemÃ¡tico**: ComparaciÃ³n de Zero-Shot vs Chain-of-Thought
2. **AnÃ¡lisis de Sentimiento**: Few-Shot learning para clasificaciÃ³n de texto
3. **ResoluciÃ³n de Problemas**: ReAct para tareas que requieren razonamiento iterativo
4. **Razonamiento LÃ³gico**: EvaluaciÃ³n completa de mÃºltiples tÃ©cnicas

## ğŸ“ˆ MÃ©tricas de EvaluaciÃ³n

El toolkit incluye:
- **Tiempo de ejecuciÃ³n**: MediciÃ³n del tiempo de respuesta
- **Conteo de tokens**: AnÃ¡lisis de eficiencia
- **Scores personalizados**: MÃ©tricas especÃ­ficas por tarea
- **Reportes comparativos**: AnÃ¡lisis detallado de rendimiento

## ğŸ¨ Dashboard Interactivo con Streamlit

El proyecto incluye una aplicaciÃ³n web interactiva construida con Streamlit que permite:

### CaracterÃ­sticas del Dashboard:
- ğŸ“Š **Visualizaciones Interactivas**: GrÃ¡ficos de barras comparativos usando Plotly
- â±ï¸ **MÃ©tricas en Tiempo Real**: Tiempo de ejecuciÃ³n, tokens y scores
- ğŸ”„ **EjecuciÃ³n de Benchmarks**: Ejecuta benchmarks directamente desde la interfaz
- ğŸ“‹ **Tablas Filtrables**: Filtra resultados por tÃ©cnica o tarea
- ğŸ“¥ **ExportaciÃ³n de Datos**: Descarga resultados en formato CSV
- ğŸ¯ **Comparaciones Visuales**: Compara mÃºltiples tÃ©cnicas lado a lado

### CÃ³mo usar el Dashboard:

1. **Instalar dependencias**:
   ```bash
   pip install -r requirements.txt
   ```

2. **Ejecutar la aplicaciÃ³n**:
   ```bash
   streamlit run streamlit_app.py
   ```

3. **En el navegador**:
   - Selecciona las tÃ©cnicas a evaluar en la barra lateral
   - Haz clic en "Ejecutar Benchmarks"
   - Explora los grÃ¡ficos y tablas interactivas
   - Descarga los resultados si lo necesitas

### Capturas de pantalla del Dashboard:
- GrÃ¡ficos de tiempo de ejecuciÃ³n por tÃ©cnica
- ComparaciÃ³n de uso de tokens
- Tablas detalladas con filtros
- MÃ©tricas resumidas en tarjetas

## ğŸ”§ ConfiguraciÃ³n Avanzada

Puedes personalizar el comportamiento modificando `src/utils/config.py` o usando variables de entorno:

```python
from src.utils.config import Config

Config.DEFAULT_MODEL = "gpt-4-turbo"
Config.DEFAULT_TEMPERATURE = 0.5
Config.DEFAULT_MAX_TOKENS = 2000
```
## ğŸŒ Despliegue

Este proyecto puede desplegarse fÃ¡cilmente en varias plataformas:

### OpciÃ³n RÃ¡pida: Streamlit Cloud (Recomendado) â­

1. Sube tu cÃ³digo a GitHub
2. Ve a [Streamlit Cloud](https://streamlit.io/cloud)
3. Conecta tu repositorio
4. Configura tus API keys en Settings > Secrets
5. Â¡Despliega! Tu app estarÃ¡ en `https://tu-app.streamlit.app`


ğŸ“– **GuÃ­a completa de despliegue**: Ver [DEPLOYMENT.md](DEPLOYMENT.md)



